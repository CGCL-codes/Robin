# Robin: A Novel Method to Produce Robust Interpreters for Deep Learning-Based Code Classifiers

We propose a novel post-hoc approach with local interpretation, named Robin, for producing robust interpreters for a given deep learning-based code classifier. The interpreter generated by Robin can identify the features in the input code examples that are important to make accurate classifications, while also robust against perturbations in the code examples. We test Robin on two deep learning-based code classifiers: DL-CAIS for code authorship attribution and TBCNN for code functionality classification.

## Classifiers to be explained
- DL-CAIS: code authorship attrution

  > Mohammed Abuhamad, Tamer AbuHmed, Aziz Mohaisen, and DaeHun Nyang. 2018. Large-scale and language-oblivious code authorship identification. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 101â€“114.

  We use a dataset from the Google Code Jam (GCJ), involving 1,632 C++ program files from 204 authors for 8 programming challenges. This dataset is different from the one used by Abuhamad et al., which is not available to us. You can download this dataset by this link: https://github.com/EQuiw/code-imitator.

- TBCNN: code functionality classification

  > Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing. In Thirtieth AAAI conference on artificial intelligence.

  We use the dataset of pedagogical programming Open Judge (OJ) system, involving 52,000 C programs for 104 programming problems. This dataset is the same as the one used by Mou et al. because it is publicly available. You can download this dataset by this link: https://sites.google.com/site/treebasedcnn/.